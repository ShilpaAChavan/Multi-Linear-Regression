####################Performing Clustering on reduced dimension after applying pca
# Normalizing the data
norm_clus<-scale(clus_data) # Scale function is used to normalize data
#Data : wine.csv
#####################################################################################
wineData <- read.csv(file.choose()) #wine.csv
View(wineData)
View(wineData)
## the first column in mydata has type
View(wineData[-1])
pcaObj<-princomp(wineData[-1], cor = TRUE, scores = TRUE, covmat = NULL)
summary(pcaObj)
loadings(pcaObj)
plot(pcaObj) # graph showing importance of principal components
pcaObj$scores #- this is to check the scores of your principal components
# cbind used to bind the data in column wise
# Considering top 3 principal component scores and binding them with mydata
wineDataNScore<-cbind(wineData,pcaObj$scores[,1:3])
View(wineDataNScore)
# preparing data for clustering (considering only pca scores as they represent the entire data)
clus_data<-wineData[,15:17]
# cbind used to bind the data in column wise
# Considering top 3 principal component scores and binding them with mydata
wineDataNScore<-cbind(wineData,pcaObj$scores[,1:3])
View(wineDataNScore)
# preparing data for clustering (considering only pca scores as they represent the entire data)
clus_data<-wineData[,15:17]
View(wineDataNScore)
# preparing data for clustering (considering only pca scores as they represent the entire data)
clus_data<-wineDataNScore[,15:17]
View(clus_data)
####################Performing Clustering on reduced dimension after applying pca
# Normalizing the data
norm_clus<-scale(clus_data) # Scale function is used to normalize data
dist1<-dist(norm_clus,method = "euclidean") # method for finding the distance
# Clustering the data using hclust function --> Hierarchical
fit1<-hclust(dist1,method="complete") # method here is complete linkage
plot(fit1, hang=-1) # Displaying Dendrogram
groups<-cutree(fit1,5) # Cutting the dendrogram for 5 clusters
membership_1<-as.matrix(groups) # cluster numbering
View(membership_1)
final1<-cbind(membership_1,wineData) # binding column wise with orginal data
View(final1)
mydata<-read.csv("D:\\Shilpa\\Datascience\\classroom\\varun- R\\PCA\\Universities_clustering.csv") ## use read.csv for csv files
View(mydata)
## the first column in mydata has university names
View(mydata[-1])
# mydata[-1] -> Considering only numerical values for applying PCA
#data <- mydata[-1]
#attach(data)
#cor(data)
pcaObj<-princomp(mydata[-1], cor = TRUE, scores = TRUE, covmat = NULL)
?princomp
## princomp(mydata, cor = TRUE) not_same_as prcomp(mydata, scale=TRUE); similar, but different
summary(pcaObj)
loadings(pcaObj)
plot(pcaObj) # graph showing importance of principal components
#pcaObj$loadings
pcaObj$scores #- this is to check the scores of your principal components
pcaObj$scores[,1:3] # Top 3 PCA Scores which represents the whole data
# cbind used to bind the data in column wise
# Considering top 3 principal component scores and binding them with mydata
mydata<-cbind(mydata,pcaObj$scores[,1:3])
View(mydata)
# preparing data for clustering (considering only pca scores as they represent the entire data)
clus_data<-mydata[,8:10]
clus_data
# Normalizing the data
norm_clus<-scale(clus_data) # Scale function is used to normalize data
dist1<-dist(norm_clus,method = "euclidean") # method for finding the distance
# Clustering the data using hclust function --> Hierarchical
fit1<-hclust(dist1,method="complete") # method here is complete linkage
plot(fit1, hang=-1) # Displaying Dendrogram
groups<-cutree(fit1,5) # Cutting the dendrogram for 5 clusters
membership_1<-as.matrix(groups) # cluster numbering
View(membership_1)
final1<-cbind(membership_1,mydata) # binding column wise with orginal data
View(final1)
####################Performing Clustering on reduced dimension after applying pca
# Normalizing the data
norm_clus<-scale(clus_data) # Scale function is used to normalize data
dist1<-dist(norm_clus,method = "euclidean") # method for finding the distance
# Clustering the data using hclust function --> Hierarchical
fit1<-hclust(dist1,method="complete") # method here is complete linkage
plot(fit1, hang=-1) # Displaying Dendrogram
groups<-cutree(fit1,5) # Cutting the dendrogram for 5 clusters
membership_1<-as.matrix(groups) # cluster numbering
View(aggregate(final1[,-c(2,9:11)],by=list(membership_1),FUN=mean)) # Inferences can be
View(membership_1)
membership_1<-as.matrix(groups) # cluster numbering
View(membership_1)
final1<-cbind(membership_1,mydata) # binding column wise with orginal data
View(final1)
View(aggregate(final1[,-c(2,9:11)],by=list(membership_1),FUN=mean)) # Inferences can be
#Data : wine.csv
#####################################################################################
wineData <- read.csv(file.choose()) #wine.csv
View(wineData)
?princomp ## to understand the api for princomp
## the first column in mydata has type
View(wineData[-1])
pcaObj<-princomp(wineData[-1], cor = TRUE, scores = TRUE, covmat = NULL)
summary(pcaObj)
loadings(pcaObj)
plot(pcaObj) # graph showing importance of principal components
pcaObj$scores #- this is to check the scores of your principal components
pcaObj$scores[,1:3]
# cbind used to bind the data in column wise
# Considering top 3 principal component scores and binding them with mydata
wineDataNScore<-cbind(wineData,pcaObj$scores[,1:3])
View(wineDataNScore)
# preparing data for clustering (considering only pca scores as they represent the entire data)
clus_data<-wineDataNScore[,15:17]
View(clus_data)
####################Performing Clustering on reduced dimension after applying pca
# Normalizing the data
norm_clus<-scale(clus_data) # Scale function is used to normalize data
dist1<-dist(norm_clus,method = "euclidean") # method for finding the distance
# Clustering the data using hclust function --> Hierarchical
fit1<-hclust(dist1,method="complete") # method here is complete linkage
plot(fit1, hang=-1) # Displaying Dendrogram
groups<-cutree(fit1,5) # Cutting the dendrogram for 5 clusters
membership_1<-as.matrix(groups) # cluster numbering
View(membership_1)
final1<-cbind(membership_1,wineData) # binding column wise with orginal data
View(final1)
View(aggregate(final1[,-c(2,16:18)],by=list(membership_1),FUN=mean))  # Inferences can be
FinalGrp
FinalGrp <-aggregate(final1[,-c(2,16:18)],by=list(membership_1),FUN=mean)
FinalGrp
#######Performing hierarchial clustering on original wines data ##############################
# Normalizing the data
normWineData<-scale(wineData[-1]) # Scale function is used to normalize data
dist2<-dist(normWineData,method = "euclidean") # method for finding the distance
# Clustering the data using hclust function --> Hierarchical
fit2<-hclust(dist2,method="complete") # method here is complete linkage
plot(fit2, hang=-1)
# Displaying Dendrogram
groups<-cutree(fit2,5)
# Cutting the dendrogram for 5 clusters
membership_2<-as.matrix(groups)
# cluster numbering
View(membership_2)
final2<-cbind(membership_2,wineData)
# binding column wise with orginal data
View(final2)
FinalGrp2 <-aggregate(final1[,-c(2,16:18)],by=list(membership_2),FUN=mean)
FinalGrp2
View(FinalGrp2)  # Inferences can be
final2<-cbind(membership_2,wineData)
# binding column wise with orginal data
View(final2)
FinalGrp2 <-aggregate(final2[,-c(2,16:18)],by=list(membership_2),FUN=mean)
FinalGrp2
View(FinalGrp2)  # Inferences can be
# preparing data for clustering (considering only pca scores as they represent the entire data)
clus_data<-wineDataNScore[,15:17]
View(clus_data)
c
####################Performing Hierarchial Clustering on reduced dimension after applying pca########
# Normalizing the data
norm_clus<-scale(clus_data) # Scale function is used to normalize data
dist1<-dist(norm_clus,method = "euclidean") # method for finding the distance
# Clustering the data using hclust function --> Hierarchical
fit1<-hclust(dist1,method="complete") # method here is complete linkage
plot(fit1, hang=-1)
# Displaying Dendrogram
groups<-cutree(fit1,7)
# Cutting the dendrogram for 7 clusters
membership_1<-as.matrix(groups)
# cluster numbering
View(membership_1)
final1<-cbind(membership_1,wineData)
# binding column wise with orginal data
View(final1)
FinalGrp <-aggregate(final1[,-c(2,16:18)],by=list(membership_1),FUN=mean)
FinalGrp
View(FinalGrp)  # Inferences can be
#######Performing hierarchial clustering on original wines data ##############################
# Normalizing the data
normWineData<-scale(wineData[-1]) # Scale function is used to normalize data
dist2<-dist(normWineData,method = "euclidean") # method for finding the distance
# Clustering the data using hclust function --> Hierarchical
fit2<-hclust(dist2,method="complete") # method here is complete linkage
plot(fit2, hang=-1)
# Cutting the dendrogram for 7 clusters
membership_2<-as.matrix(groups)
# cluster numbering
View(membership_2)
final2<-cbind(membership_2,wineData)
# binding column wise with orginal data
View(final2)
FinalGrp2 <-aggregate(final2[,-c(2,16:18)],by=list(membership_2),FUN=mean)
FinalGrp2
#################################Performing Kmeans clustering after pca dimension reduction ###
#elbow curve & k ~ sqrt(n/2) to decide the k value
install.packages("factoextra")
library(factoextra)
fviz_nbclust(wineData[-1],kmeans,method="wss")+labs(subtitle = "Elbow method")
# k clustering alternative for large dataset - Clustering Large Applications (Clara)
install.packages("cluster")
library(cluster)
# k clustering alternative for large dataset - Clustering Large Applications (Clara)
install.packages("cluster")
# k clustering alternative for large dataset - Clustering Large Applications (Clara)
install.packages("cluster")
?clara
library(cluster)
?clara
xcl <- clara(wineData, 7, sample = 100)
clusplot(xcl)
#Partitioning around medoids
xpm <- pam(wineData, 7)
clusplot(xpm)
fviz_nbclust(clus_data,kmeans,method="wss")+labs(subtitle = "Elbow method")
library(factoextra)
fviz_nbclust(clus_data,kmeans,method="wss")+labs(subtitle = "Elbow method")
# k clustering for large dataset - Clustering Large Applications (Clara)
install.packages("cluster")
install.packages("cluster")
library(cluster)
?clara
xcl <- clara(clus_data, 7, sample = 100)
xcl <- clara(clus_data, 7, sample = 100)
clusplot(xcl)
#Partitioning around medoids
xpm <- pam(clus_data, 7)
clusplot(xpm)
library(psych)
#to transform data from character to numeric
install.packages("plyr")
library(plyr)
library(car)
ComputerDetails <- read.csv(file.choose()) #Computer_Data.csv
ComputerData <- ComputerDetails[2:11]
ComputerData$cd <- as.numeric(revalue(ComputerData$cd,c("yes"=1, "no"=0)))
ComputerData$multi <- as.numeric(revalue(ComputerData$multi,c("yes"=1, "no"=0)))
ComputerData$premium <- as.numeric(revalue(ComputerData$premium,c("yes"=1, "no"=0)))
attach(ComputerData)
View(ComputerData)
summary(ComputerData)
model.computerDataLog <- lm(price ~ log(speed)+log(hd)+log(ram)+log(screen)+log(cd)+log(multi)+log(premium)
+log(ads)+log(trend), data=ComputerData[-c(1440,1701),] )
summary(model.computerDataLog)
##Exponential Transformation
model.computerDataExp <-lm(log(price) ~ speed+hd+ram+screen+cd+multi+premium+ads+trend, data=ComputerData[-c(1440,1701),])
##Exponential Transformation
model.computerDataExp <-lm(log(price) ~ speed+hd+ram+screen+cd+multi+premium+ads+trend, data=ComputerData[-c(1440,1701),])
summary(model.computerDataExp)
model.computerDataQuad<-lm(price ~ speed+hd+ram+screen+cd+multi+premium+ads+trend
+I(speed^2)+I(hd^2)+I(ram^2)+I(screen^2)+I(cd^2)+I(multi^2)+I(premium^2)
+I(ads^2)+I(trend^2), data=ComputerData[-c(1440,1701),])
summary(model.computerDataQuad)
#Poly model transformatio
model.computerDataPoly<-lm(price ~ speed+hd+ram+screen+cd+multi+premium+ads+trend
+I(speed^2)+I(hd^2)+I(ram^2)+I(screen^2)+I(cd^2)+I(multi^2)+I(premium^2)
+I(ads^2)+I(trend^2) +I(speed^3)+I(hd^3)+I(ram^3)+I(screen^3)+I(cd^3)+I(multi^3)+I(premium^2)
+I(ads^3)+I(trend^3), data=ComputerData[-c(1440,1701),])
summary(model.computerDataPoly)
### Variance Inflation Factors
vif(model.computerDataPoly)  # VIF is > 10 => collinearity
### Variance Inflation Factors
vif(model.computerDataPoly)  # VIF is > 10 => collinearity
summary(model.computerDataPoly)
library(car)
# Diagnostic Plots
install.packages("car")
library(car)
### Variance Inflation Factors
vif(model.computerDataPoly)  # VIF is > 10 => collinearity
### Variance Inflation Factors
vif(model.computerDataPoly)  # VIF is > 10 => collinearity
### Variance Inflation Factors
vif(model.computerDataPoly)  # VIF is > 10 => collinearity
computerDataPoly <- model.computerDataPoly
summary(computerDataPoly)
### Variance Inflation Factors
vif(computerDataPoly)  # VIF is > 10 => collinearity
#Poly model transformation
model.computerDataPoly<-lm(price ~ speed+hd+ram+screen+cd+multi+premium+ads+trend
+I(speed^2)+I(hd^2)+I(ram^2)+I(screen^2)+I(cd^2)+I(multi^2)+I(premium^2)
+I(ads^2)+I(trend^2) +I(speed^3)+I(hd^3)+I(ram^3)+I(screen^3)+I(cd^3)+I(multi^3)+I(premium^2)
+I(ads^3)+I(trend^3), data=ComputerData[-c(1440,1701),])
fit <- model.computerDataPoly
#the linearly dependent variables
ld.vars <- attributes(alias(fit)$Complete)$dimnames[[1]]
#remove the linearly dependent variables variables
formula.new <- as.formula(
paste(
paste(deparse(formula), collapse=""),
paste(ld.vars, collapse="-"),
sep="-"
)
)
fit <- model.computerDataPoly
#the linearly dependent variables
ld.vars <- attributes(alias(fit)$Complete)$dimnames[[1]]
#remove the linearly dependent variables variables
formula.new <- as.formula(
paste(
paste(deparse(formula), collapse=""),
paste(ld.vars, collapse="-"),
sep="-"
)
)
### Variance Inflation Factors
vif(computerModel)  # VIF is > 10 => collinearity
#Regression model after deleting the 1441 & 1701 observation
computerModel <- lm(price ~ speed+hd+ram+screen+cd+multi+premium+ads+trend, data=ComputerData[-c(1440,1701),])
summary(computerModel)
### Variance Inflation Factors
vif(computerModel)  # VIF is > 10 => collinearity
summary(FinalModel)
#Poly model transformation
model.computerDataPoly<-lm(price ~ speed+hd+ram+screen+cd+multi+premium+ads+trend
+I(speed^2)+I(hd^2)+I(ram^2)+I(screen^2)+I(cd^2)+I(multi^2)+I(premium^2)
+I(ads^2)+I(trend^2) +I(speed^3)+I(hd^3)+I(ram^3)+I(screen^3)+I(cd^3)+I(multi^3)+I(premium^2)
+I(ads^3)+I(trend^3), data=ComputerData[-c(1440,1701),])
FinalModel <- model.computerDataPoly
summary(FinalModel)
plot(FinalModel)
FinalModel
summary(FinalModel)
plot(FinalModel)
#Poly model transformation
FinalModel<-lm(price ~ speed+hd+ram+screen+cd+multi+premium+ads+trend
+I(speed^2)+I(hd^2)+I(ram^2)+I(screen^2)+I(cd^2)+I(multi^2)+I(premium^2)
+I(ads^2)+I(trend^2) +I(speed^3)+I(hd^3)+I(ram^3)+I(screen^3)+I(cd^3)+I(multi^3)+I(premium^2)
+I(ads^3)+I(trend^3), data=ComputerData[-c(1440,1701),])
summary(FinalModel)
plot(FinalModel)
qqplot((FinalModel),id.n=5)
qqplot((FinalModel),id.n=5)
qqplot((FinalModel))
install.packages("lattice")
library("lattice")
qqplot((FinalModel))
qqPlot(FinalModel)
library(MASS)
stepAIC(computerModel)
### Variance Inflation Factors
vif(FinalModel)  # VIF is > 10 => collinearity
avPlots(computerModel, id.n=2, id.cex=0.8, col="red")
avPlots(computerModel, id.n=2, id.cex=0.8, col="lightblue")
avPlots(computerModel, id.n=2, id.cex=0.8, col="lightblue")
library(psych)
#to transform data from character to numeric
install.packages("plyr")
library(plyr)
library(car)
ComputerDetails <- read.csv(file.choose()) #Computer_Data.csv
ComputerData <- ComputerDetails[2:11]
ComputerData$cd <- as.numeric(revalue(ComputerData$cd,c("yes"=1, "no"=0)))
ComputerData$multi <- as.numeric(revalue(ComputerData$multi,c("yes"=1, "no"=0)))
ComputerData$premium <- as.numeric(revalue(ComputerData$premium,c("yes"=1, "no"=0)))
attach(ComputerData)
View(ComputerData)
#Poly model transformation
FinalModel<-lm(price ~ speed+hd+ram+screen+cd+multi+premium+ads+trend
+I(speed^2)+I(hd^2)+I(ram^2)+I(screen^2)+I(cd^2)+I(multi^2)+I(premium^2)
+I(ads^2)+I(trend^2) +I(speed^3)+I(hd^3)+I(ram^3)+I(screen^3)+I(cd^3)+I(multi^3)+I(premium^2)
+I(ads^3)+I(trend^3), data=ComputerData[-c(1440,1701),])
summary(FinalModel)
stepAIC(FinalModel)
library(MASS)
stepAIC(FinalModel)
library(psych)
# Diagnostic Plots
install.packages("car")
#to transform data from character to numeric
install.packages("plyr")
library(plyr)
library(car)
ComputerDetails <- read.csv(file.choose()) #Computer_Data.csv
ComputerData <- ComputerDetails[2:11]
ComputerData$cd <- as.numeric(revalue(ComputerData$cd,c("yes"=1, "no"=0)))
ComputerData <- ComputerDetails[2:11]
ComputerData$cd <- as.numeric(revalue(ComputerData$cd,c("yes"=1, "no"=0)))
ComputerData$multi <- as.numeric(revalue(ComputerData$multi,c("yes"=1, "no"=0)))
ComputerData$premium <- as.numeric(revalue(ComputerData$premium,c("yes"=1, "no"=0)))
attach(ComputerData)
View(ComputerData)
summary(ComputerData)
#Building the linear regression model
computerModel <- lm(price ~ speed+hd+ram+screen+cd+multi+premium+ads+trend)
summary(computerModel)
library(MASS)
stepAIC(computerModel)
